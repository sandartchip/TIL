
### Affine Layer - Backward 

```python

def eval_numerical_gradient(f, x, verbose=True, h=0.00001):
    """
    a naive implementation of numerical gradient of f at x
    - f should be a function that takes a single argument
    - x is the point (numpy array) to evaluate the gradient at
    """
    # 함수 f에 대한 특정 지점 x에서의 미분값을 계산함.

    fx = f(x)  # evaluate function value at original point
    grad = np.zeros_like(x)
    
    # iterate over all indexes in x
    it = np.nditer(x, flags=["multi_index"], op_flags=["readwrite"]) # np.nditer : 다차원 배열을 iterate하는 numpy 전용 함수.  
    while not it.finished:

        # evaluate function at x+h
        ix = it.multi_index # multi_index는 다차원 배열에서의 좌표를 가리키는 튜플임. 
        oldval = x[ix]
        x[ix] = oldval + h  # increment by h. ?? h는 변수 지정 안되있는데 이가 뭔가요..? 상수인가요..??
        fxph = f(x)  # evalute f(x + h)
        x[ix] = oldval - h
        fxmh = f(x)  # evaluate f(x - h)
        x[ix] = oldval  # restore

        # compute the partial derivative(편미분) with centered formula
        # 위에 x와 f
        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope
        if verbose:
            print(ix, grad[ix])
        it.iternext()  # step to next dimension

    return grad
```

### 계산식 
#### dx
<img src="https://github.com/sandartchip/TIL/assets/15938354/af0cedd4-ecdb-4111-819d-f7924d024014" />

#### dW
![image](https://github.com/sandartchip/TIL/assets/15938354/e5b09603-1b93-42c9-897d-17685c48d052)

Y는 뭔가요.....?


#### db


<img src="https://github.com/sandartchip/TIL/assets/15938354/01dc7f21-005d-4def-9dc9-099e94335967" width="500px"/>

- For the backward pass over the fully connected layers, we need to calculate the gradient of **out** with respect to W, x and b.
- Let's take at a look at the circuit diagram representing the fully-connected neural layers.
 

출처:
https://usmanr149.github.io/urmlblog/cs231n%20assignments/2020/03/29/FC-NN.html
