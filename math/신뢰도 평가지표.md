
## 카파 상관계수

![image](https://user-images.githubusercontent.com/15938354/229427315-a425652e-af90-4fef-8c88-1bf0f08a5d6d.png)


- cohen kappa : 코헨의 카파 상관계수를 가리킴.

- 2명의 관찰자간의 신뢰도를 확보하기 위한 확률임. 
- 0~0.2 사이 -> slight agreement, 약간의 일치도. 
- 0.8 이상 -> almost perfect agreement, 완벽한 일치도.

- 두 관찰자 간의 측정범주 값에 대한 일치도를 측정함. 
- 세 관찰자 이상 일치도 -> Fleiss Kappa 


![image](https://user-images.githubusercontent.com/15938354/229428300-761f614c-b110-40d1-86cb-c25f677da6a7.png)

- 2명의 판독의가 100명의 유방암 사진을 보면서 양성과 악성을 판별할 때, 
- 2명의 판독의가 모두 양성이라고 한 것은 50개, 모두 악성이라고 한 것은 30개. 
- Pa: 판독의1과 판독의2가 일치할 확률. 
- Pc: 우연히 두 평가자에 의하여 일치된 평가를 받을 확률

![image](https://user-images.githubusercontent.com/15938354/229428313-281478df-c4d5-46d8-b443-cb5cee30b6fd.png)

![image](https://user-images.githubusercontent.com/15938354/229430095-287391ab-f951-4545-a514-4462e9be92bb.png)


![image](https://user-images.githubusercontent.com/15938354/229430448-38d858f0-c9f6-439a-a66b-9fdbc5d8cfe3.png)
- 판독의1과 2 사이의 일치도가 0.583 -> Landis and koch의 해석에 따르면 moderate의 일치도.

Pa도 평가자간 일치 확률이고 K도 평가자간 일치 확률인데 뭐여 이거..

https://blog.naver.com/y4769/220680837692
