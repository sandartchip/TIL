
#### TORCH.AUTOGRAD

- 신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파이다.
- 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수(Loss Function)의 변화도(gradient)에 따라 조정된다.

- 이러한 변화도를 계산하기 위해, PyTorch에서는 torch.autograd라고 불리는 자동미분 엔진이 내장되어 있다.
- 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원한다.

- 입력 x, 매개변수 w와 b, 그리고 일부 손실함수가 있는 가장 간단한 단일 계층 신경망을 가정.

#### torch.autograd.Variable 

![image](https://user-images.githubusercontent.com/15938354/175445588-41a03218-e6ab-4ba4-b5c6-774c64ed0c60.png)

- Variable 클래스는, 각 tensor의 값을 볼 수 있는 data, 미분을 보는 grad, backward를 통한 미분을 계산한 함수 정보인 grad_fn 총 3개의 함수를 사용 가능.

- 즉, loss의 backward()를 통해 backpropagation을 구하고 이를 통해 weight들을 자신의 데이터에 맞게 변화시킴

## require_grad 
- **require_grad=True** 로 설정하면, 역전파 중에 이 Tensor들에 대한 변화도를 계산하라는 의미가 된다.
- 계산한 변화도는, Tensor 내에 저장된다. 

## loss.backward()
- 역전파 과정에서, 각 노드의 변화도를 계산하는 것. 
- 이 함수는, backpropagation을 수행하여 x.grad에 변화도를 저장한다.
- x.grad에 저장된 변화도를 이용하여 파라미터 값을 갱신함. 
- 변화도를 계산한다는 것은, 곧 파라미터 값을 업데이트하는 것이고,
- 변화도를 계산하지 않겠다는 것은 파라미터 값을 업데이트 하지 않겠다는 의미이다.
